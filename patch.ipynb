{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5fd700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 12:59:38.928812: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 7.48 GiB (download: 7.48 GiB, generated: Unknown size, total: 7.48 GiB) to /Users/melody.zhao/tensorflow_datasets/patch_camelyon/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6560f5fe7e1240d1ad338ee4d0cf7882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f7d5dcaec04fed9996d6ae401edf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cd0340b5c8464b90e09225f3d8fbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/32768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 13:18:18.024094: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling patch_camelyon-test.tfrecord...:   0%|          | 0/32768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/262144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling patch_camelyon-train.tfrecord...:   0%|          | 0/262144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/32768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling patch_camelyon-validation.tfrecord...:   0%|          | 0/32768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset patch_camelyon downloaded and prepared to /Users/melody.zhao/tensorflow_datasets/patch_camelyon/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='patch_camelyon',\n",
      "    full_name='patch_camelyon/2.0.0',\n",
      "    description=\"\"\"\n",
      "    The PatchCamelyon benchmark is a new and challenging image classification\n",
      "    dataset. It consists of 327.680 color images (96 x 96px) extracted from\n",
      "    histopathologic scans of lymph node sections. Each image is annoted with a\n",
      "    binary label indicating presence of metastatic tissue. PCam provides a new\n",
      "    benchmark for machine learning models: bigger than CIFAR10, smaller than\n",
      "    Imagenet, trainable on a single GPU.\n",
      "    \"\"\",\n",
      "    homepage='https://patchcamelyon.grand-challenge.org/',\n",
      "    data_path='/Users/melody.zhao/tensorflow_datasets/patch_camelyon/2.0.0',\n",
      "    download_size=7.48 GiB,\n",
      "    dataset_size=7.06 GiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(96, 96, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=32768, num_shards=8>,\n",
      "        'train': <SplitInfo num_examples=262144, num_shards=64>,\n",
      "        'validation': <SplitInfo num_examples=32768, num_shards=8>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{b_s_veeling_j_linmans_j_winkens_t_cohen_2018_2546921,\n",
      "      author       = {B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling},\n",
      "      title        = {Rotation Equivariant CNNs for Digital Pathology},\n",
      "      month        = sep,\n",
      "      year         = 2018,\n",
      "      doi          = {10.1007/978-3-030-00934-2_24},\n",
      "      url          = {https://doi.org/10.1007/978-3-030-00934-2_24}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "pcam, pcam_info = tfds.load(\"patch_camelyon\", with_info=True)\n",
    "print(pcam_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4c832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TensorFlow and relevant Keras classes to setup the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578a3407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 94, 94, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 92, 92, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 46, 46, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 19, 19, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 17, 17, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,154,066\n",
      "Trainable params: 1,154,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melody.zhao/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(96,96,3))\n",
    "\n",
    "# Now we define the layers of the convolutional network: three blocks of two convolutional layers and a max-pool layer.\n",
    "x = Conv2D(16, (3, 3), padding='valid', activation='relu')(input_img)\n",
    "x = Conv2D(16, (3, 3), padding='valid', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "x = Conv2D(32, (3, 3), padding='valid', activation='relu')(x)\n",
    "x = Conv2D(32, (3, 3), padding='valid', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n",
    "x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "\n",
    "# Now we flatten the output from a 4D to a 2D tensor to be able to use fully-connected (dense) layers for the final\n",
    "# classification part. Here we also use a bit of dropout for regularization. The last layer uses a softmax to obtain class\n",
    "# likelihoods (i.e. metastasis vs. non-metastasis)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(rate=0.2)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Now we define the inputs/outputs of the model and setup the optimizer. In this case we use regular stochastic gradient\n",
    "# descent with Nesterov momentum. The loss we use is cross-entropy and we would like to output accuracy as an additional metric.\n",
    "model = Model(inputs=input_img, outputs=predictions)\n",
    "sgd_opt = SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "model.compile(optimizer=sgd_opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7b6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep the dataset size small PatchCAMELYON is stored as int8 patches. \n",
    "# For network training we need float32 and we want to normalize between 0 and 1. \n",
    "# The function below performs this task.\n",
    "\n",
    "\n",
    "def convert_sample(sample):\n",
    "    image, label = sample['image'], sample['label']  \n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.one_hot(label, 2, dtype=tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96599d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the training data with a shuffle buffer \n",
    "# batches of 64 patches for training and 128 for validation\n",
    "# prefetch 2 batches such that we can get batches during training on the GPU\n",
    "\n",
    "train_pipeline = pcam['train'].map(convert_sample,\n",
    "                                   num_parallel_calls=8).shuffle(1024).repeat().batch(64).prefetch(2)\n",
    "valid_pipeline = pcam['validation'].map(convert_sample,\n",
    "                                        num_parallel_calls=8).repeat().batch(128).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300870a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 14:53:17.051142: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4096 - 1249s - loss: 0.4464 - accuracy: 0.7818 - val_loss: 0.3864 - val_accuracy: 0.8252\n",
      "Epoch 2/5\n",
      "4096/4096 - 1250s - loss: 0.3151 - accuracy: 0.8676 - val_loss: 0.3782 - val_accuracy: 0.8277\n",
      "Epoch 3/5\n",
      "4096/4096 - 1236s - loss: 0.2652 - accuracy: 0.8919 - val_loss: 0.3690 - val_accuracy: 0.8355\n",
      "Epoch 4/5\n",
      "4096/4096 - 1187s - loss: 0.2314 - accuracy: 0.9085 - val_loss: 0.3809 - val_accuracy: 0.8322\n",
      "Epoch 5/5\n",
      "4096/4096 - 1198s - loss: 0.2072 - accuracy: 0.9190 - val_loss: 0.3459 - val_accuracy: 0.8548\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_pipeline,\n",
    "                 validation_data=valid_pipeline,\n",
    "                 verbose=2, epochs=5, steps_per_epoch=4096, validation_steps=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61dca2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is 0.8135\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = pcam['test'].map(convert_sample, num_parallel_calls=8).batch(128).prefetch(2)\n",
    "print(\"Test set accuracy is {0:.4f}\".format(model.evaluate(test_pipeline, steps=128, verbose=0)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93f8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 16:52:55.329975: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./patchcamelyon.hf5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./patchcamelyon.hf5/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./patchcamelyon.hf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33bf77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
